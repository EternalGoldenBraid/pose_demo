{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "useful-subscription",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "\n",
    "\n",
    "from os.path import join as pjoin\n",
    "from bop_toolkit_lib import inout\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base_path = os.path.dirname(os.path.abspath(\".\"))\n",
    "sys.path.append(base_path)\n",
    "\n",
    "from lib import rendering, network\n",
    "\n",
    "from dataset import LineMOD_Dataset, prototype_Dataset\n",
    "from evaluation import utils\n",
    "from evaluation import config as cfg\n",
    "\n",
    "gpu_id = 0\n",
    "# gpu_id = 1\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "os.environ['EGL_DEVICE_ID'] = str(gpu_id)\n",
    "DEVICE = torch.device('cuda')\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "\n",
    "\n",
    "datapath = Path(cfg.DATA_PATH)\n",
    "#eval_dataset = prototype_Dataset.Dataset(datapath / 'huawei_box')\n",
    "eval_dataset = LineMOD_Dataset.Dataset(datapath / 'lm')\n",
    "\n",
    "cfg.RENDER_WIDTH = eval_dataset.cam_width    # the width of rendered images\n",
    "cfg.RENDER_HEIGHT = eval_dataset.cam_height  # the height of rendered images\n",
    "#cfg.DATASET_NAME = 'huawei_box'        # dataset name\n",
    "cfg.DATASET_NAME = 'lm'        # dataset name\n",
    "\n",
    "cfg.HEMI_ONLY = True   # only the upper hemishpere is used for LineMOD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d46f50",
   "metadata": {},
   "source": [
    "# Load Mask-RCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd464b03-aed8-41b9-be10-a460c34d726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not needed for image test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573ed547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask-RCNN has been loaded!\n"
     ]
    }
   ],
   "source": [
    "################################################ MASK-RCNN Segmentation ##################################################################\n",
    "rcnnIdx_to_lmIds_dict = {0:1, 1:2, 2:3, 3:4, 4:5, 5:6, 6:7, 7:8, 8:9, 9:10, 10:11, 11:12, 12:13, 13:14, 14:15}\n",
    "rcnnIdx_to_lmCats_dict ={0:'Ape', 1:'Benchvice', 2:'Bowl', 3:'Camera', 4:'Can', 5:'Cat', 6:'Cup', 7:'Driller', \n",
    "                        8:'Duck', 9:'Eggbox', 10:'Glue', 11:'Holepunch', 12:'Iron', 13:'Lamp', 14:'Phone'}\n",
    "rcnn_cfg = get_cfg()\n",
    "rcnn_cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "rcnn_cfg.MODEL.WEIGHTS = os.path.abspath(os.path.join(base_path, 'checkpoints','lm_maskrcnn_model.pth'))\n",
    "rcnn_cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(rcnnIdx_to_lmCats_dict)\n",
    "rcnn_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.001 # the predicted category scores\n",
    "predictor = DefaultPredictor(rcnn_cfg)\n",
    "################################################# MASK-RCNN Segmentation ##################################################################\n",
    "print('Mask-RCNN has been loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65717a2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load OVE6D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24195e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVE6D has been loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ckpt_file = pjoin(base_path, \n",
    "                'checkpoints', \n",
    "                \"OVE6D_pose_model.pth\"\n",
    "                )\n",
    "model_net = network.OVE6D().to(DEVICE)\n",
    "\n",
    "model_net.load_state_dict(torch.load(ckpt_file))\n",
    "model_net.eval()\n",
    "print('OVE6D has been loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9571f",
   "metadata": {},
   "source": [
    "#  Load object viewpoint codebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9788e898-d3ef-44cf-bdcc-56d0e12419cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataspace/lm/models_eval\n",
      "<module 'evaluation.config' from '/home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/config.py'>\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset.model_dir)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e2da40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_01_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_02_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_03_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_04_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_05_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_06_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_07_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_08_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_09_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_10_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_11_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_12_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_13_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_14_views_4000.npy\n",
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/lm/zoom_0.01/views_4000/lm_obj_15_views_4000.npy\n",
      "Object codebooks have been loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg.VIEWBOOK_BATCHSIZE = 200 # reduce this if out of GPU memory, \n",
    "codebook_saving_dir = pjoin(base_path,'evaluation/object_codebooks',\n",
    "                            cfg.DATASET_NAME, \n",
    "                            'zoom_{}'.format(cfg.ZOOM_DIST_FACTOR), \n",
    "                            'views_{}'.format(str(cfg.RENDER_NUM_VIEWS)))\n",
    "\n",
    "\n",
    "\n",
    "object_codebooks = utils.OVE6D_codebook_generation(codebook_dir=codebook_saving_dir, \n",
    "                                                    model_func=model_net,\n",
    "                                                    dataset=eval_dataset, \n",
    "                                                    config=cfg, \n",
    "                                                    device=DEVICE)\n",
    "print('Object codebooks have been loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b98515b-a55b-440e-b4c7-f4e5eb99eb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11903610903064521 0.09175967025756837\n"
     ]
    }
   ],
   "source": [
    "#print(eval_dataset.obj_model_file)\n",
    "obj_mesh = object_codebooks[1]['obj_mesh']\n",
    "print(obj_mesh.bounding_diameter, obj_mesh.bounding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdcb098",
   "metadata": {},
   "source": [
    "# Object segmentation and pose estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33a0eed3-6c84-48a5-b6ac-b33020c21e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 128, 60]) torch.Size([50, 128, 60])\n",
      "torch.Size([50, 60, 60])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "b = 50\n",
    "c = 128\n",
    "d = 60\n",
    "\n",
    "x = t.rand(b,c,d).to('cuda')\n",
    "y = t.rand(b,c,d).to('cuda')\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "z = t.bmm(x.permute(0,2,1), y)\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39410384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#import ipdb; ipdb.set_trace();\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m pose_ret, rcnn_idx \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOVE6D_rcnn_full_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mobj_depths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtar_obj_depths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mobj_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtar_obj_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mobj_rcnn_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtar_obj_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mobj_codebook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtar_obj_codebook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcam_K\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcam_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mobj_renderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj_renderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mreturn_rcnn_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m obj_renderer\n\u001b[1;32m     96\u001b[0m raw_pose_R \u001b[38;5;241m=\u001b[39m pose_ret[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_R\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# without ICP\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/pose_demo/OVE6D-pose/evaluation/utils.py:481\u001b[0m, in \u001b[0;36mOVE6D_rcnn_full_pose\u001b[0;34m(model_func, obj_depths, obj_masks, obj_rcnn_scores, obj_codebook, cam_K, config, obj_renderer, device, return_rcnn_idx)\u001b[0m\n\u001b[1;32m    477\u001b[0m zoom_cost \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m zoom_timer \u001b[38;5;66;03m# the pre-processing runtime\u001b[39;00m\n\u001b[1;32m    478\u001b[0m rot_timer \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    480\u001b[0m (estimated_R, estimated_scores, \n\u001b[0;32m--> 481\u001b[0m estimated_rcnn_idx) \u001b[38;5;241m=\u001b[39m \u001b[43mOVE6D_rcnn_rotation_estimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzoom_test_depths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mrcnn_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj_rcnn_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mmodel_net\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mobject_codebook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj_codebook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    486\u001b[0m num_proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(estimated_R)\n\u001b[1;32m    487\u001b[0m init_t \u001b[38;5;241m=\u001b[39m init_ts[estimated_rcnn_idx] \n",
      "File \u001b[0;32m~/Projects/pose_demo/OVE6D-pose/evaluation/utils.py:669\u001b[0m, in \u001b[0;36mOVE6D_rcnn_rotation_estimation\u001b[0;34m(input_depth, rcnn_score, model_net, object_codebook, cfg)\u001b[0m\n\u001b[1;32m    666\u001b[0m top_codebook_z_maps \u001b[38;5;241m=\u001b[39m obj_codebook_Z_map[topK_cosim_idxes, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 669\u001b[0m     query_theta, pd_conf \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_codebook_z_maps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_obj_query_z_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mbest_obj_query_z_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_codebook_z_maps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m stn_theta \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(query_theta[:, :\u001b[38;5;241m2\u001b[39m, :\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mclone(), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    673\u001b[0m homo_z_R \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(stn_theta, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Projects/pose_demo/OVE6D-pose/lib/network.py:214\u001b[0m, in \u001b[0;36mOVE6D.inference\u001b[0;34m(self, anc_map, inp_map)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference\u001b[39m(\u001b[38;5;28mself\u001b[39m, anc_map, inp_map):\n\u001b[0;32m--> 214\u001b[0m     pd_theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregression_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manc_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     stn_inp_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_transformation(x\u001b[38;5;241m=\u001b[39manc_map, theta\u001b[38;5;241m=\u001b[39mpd_theta) \u001b[38;5;66;03m# transform anchor viewpoint with in-plane rotation\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     pd_conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewpoint_confidence(x\u001b[38;5;241m=\u001b[39minp_map, y\u001b[38;5;241m=\u001b[39mstn_inp_map)\n",
      "File \u001b[0;32m~/Projects/pose_demo/OVE6D-pose/lib/network.py:170\u001b[0m, in \u001b[0;36mOVE6D.regression_head\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    144\u001b[0m y \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(y\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(bs, ch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# z = (x.unsqueeze(3) * y.unsqueeze(2)).sum(dim=1) # BV x C x 64 x 64\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m############################################################ debug\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# BV x 64 x 64, feature map correlation\u001b[39;00m\n\u001b[1;32m    171\u001b[0m z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# BV x 4096\u001b[39;00m\n\u001b[1;32m    173\u001b[0m Rz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvp_rot_fc1(z)        \u001b[38;5;66;03m# BV x 4096 -> BV x 128\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg.VP_NUM_TOPK = 50   # the retrieval number of viewpoint \n",
    "cfg.RANK_NUM_TOPK = 5  # the ranking number of full 3D orientation \n",
    "\n",
    "test_data_dir = datapath / 'lm' / 'test'          # path to the test dataset of BOP\n",
    "obj_renderer = rendering.Renderer(width=cfg.RENDER_WIDTH, height=cfg.RENDER_HEIGHT)\n",
    "\n",
    "scene_id = 1   # range [1, 15]\n",
    "view_id = 0  # range [0, 1214]\n",
    "\n",
    "scene_dir = pjoin(test_data_dir, '{:06d}'.format(scene_id))\n",
    "color_file = pjoin(scene_dir, 'rgb/{:06d}.png'.format(view_id))\n",
    "depth_file = pjoin(scene_dir, 'depth/{:06d}.png'.format(view_id))\n",
    "mask_file = pjoin(scene_dir, 'mask_visib/{:06d}_{:06d}.png'.format(view_id, 0))\n",
    "\n",
    "tar_obj_id = scene_id # object id equals the scene id for LM dataset\n",
    "tar_obj_codebook = object_codebooks[tar_obj_id]\n",
    "\n",
    "############## read the camera information ##############\n",
    "cam_info_file = pjoin(scene_dir, 'scene_camera.json')\n",
    "with open(cam_info_file, 'r') as cam_f:\n",
    "    scene_camera_info = json.load(cam_f)\n",
    "view_cam_info = scene_camera_info[str(view_id)]  # scene camera information        \n",
    "\n",
    "\n",
    "############## read the ground truth pose for calculating errors ##############\n",
    "gt_pose_file = os.path.join(scene_dir, 'scene_gt.json')\n",
    "with open(gt_pose_file, 'r') as pose_f:\n",
    "    pose_anno = json.load(pose_f)\n",
    "view_pose = pose_anno[str(view_id)][0]\n",
    "obj_gt_R = cfg.BOP_REF_POSE(torch.tensor(view_pose['cam_R_m2c'], dtype=torch.float32).view(3, 3))\n",
    "obj_gt_t = torch.tensor(view_pose['cam_t_m2c'], dtype=torch.float32) * cfg.MODEL_SCALING\n",
    "\n",
    "\n",
    "############## read the depth images and covert it from meter to millimeter ##############\n",
    "view_depth = torch.tensor(np.array(Image.open(depth_file)), dtype=torch.float32) # HxW\n",
    "view_depth *= view_cam_info['depth_scale']\n",
    "view_depth *= cfg.MODEL_SCALING # convert to meter scale from millimeter scale\n",
    "view_camK = torch.tensor(view_cam_info['cam_K'], dtype=torch.float32).view(3, 3)[None, ...] # 1x3x3\n",
    "cam_K = view_camK.to(DEVICE)\n",
    "view_depth = view_depth.to(DEVICE)\n",
    "#import ipdb; ipdb.set_trace();\n",
    "\n",
    "\n",
    "############## read rgb image for object segmentation ##############\n",
    "\n",
    "## TODO:\n",
    "view_timer = time.time() \n",
    "rgb_img = cv2.imread(color_file)\n",
    "imread_cost = time.time() - view_timer\n",
    "\n",
    "rcnn_timer = time.time()\n",
    "output = predictor(rgb_img)\n",
    "rcnn_pred_ids = output[\"instances\"].pred_classes\n",
    "rcnn_pred_masks = output[\"instances\"].pred_masks\n",
    "rcnn_pred_scores = output[\"instances\"].scores\n",
    "rcnn_cost = time.time() - rcnn_timer\n",
    "###################### object segmentation ######################\n",
    "\n",
    "tar_rcnn_d = tar_obj_id - 1\n",
    "obj_masks = rcnn_pred_masks # NxHxW\n",
    "obj_depths = view_depth[None, ...] * obj_masks\n",
    "tar_obj_depths = obj_depths[tar_rcnn_d==rcnn_pred_ids]\n",
    "tar_obj_masks = rcnn_pred_masks[tar_rcnn_d==rcnn_pred_ids]\n",
    "tar_obj_scores = rcnn_pred_scores[tar_rcnn_d==rcnn_pred_ids]\n",
    "\n",
    "mask_pixel_count = tar_obj_masks.view(tar_obj_masks.size(0), -1).sum(dim=1)\n",
    "valid_idx = (mask_pixel_count >= 100)\n",
    "if valid_idx.sum() == 0:\n",
    "    mask_visib_ratio = mask_pixel_count / mask_pixel_count.max()\n",
    "    valid_idx = mask_visib_ratio >= 0.05\n",
    "\n",
    "tar_obj_masks = tar_obj_masks[valid_idx] # select the target object instance masks\n",
    "tar_obj_depths = tar_obj_depths[valid_idx]\n",
    "tar_obj_scores = tar_obj_scores[valid_idx]\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "ax[0].imshow(obj_masks[0].cpu().numpy())\n",
    "ax[1].imshow(obj_masks[10].cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "#import ipdb; ipdb.set_trace();\n",
    "\n",
    "pose_ret, rcnn_idx = utils.OVE6D_rcnn_full_pose(model_func=model_net, \n",
    "                                    obj_depths=tar_obj_depths,\n",
    "                                    obj_masks=tar_obj_masks,\n",
    "                                    obj_rcnn_scores=tar_obj_scores,\n",
    "                                    obj_codebook=tar_obj_codebook, \n",
    "                                    cam_K=cam_K,\n",
    "                                    config=cfg, \n",
    "                                    device=DEVICE,\n",
    "                                    obj_renderer=obj_renderer, \n",
    "                                    return_rcnn_idx=True\n",
    "                                    )\n",
    "del obj_renderer\n",
    "\n",
    "raw_pose_R = pose_ret['raw_R'] # without ICP\n",
    "raw_pose_t = pose_ret['raw_t'] # without ICP\n",
    "\n",
    "icp1_pose_R = pose_ret['icp1_R'] # with ICP after pose selection\n",
    "icp1_pose_t = pose_ret['icp1_t'] # with ICP after pose selection\n",
    "\n",
    "icpk_pose_R = pose_ret['icpk_R'] # with ICP before pose selection\n",
    "icpk_pose_t = pose_ret['icpk_t'] # with ICP before pose selection\n",
    "\n",
    "plt.imshow(tar_obj_masks.sum(dim=0).cpu())\n",
    "plt.axis(False)\n",
    "\n",
    "obj_name = rcnnIdx_to_lmCats_dict[tar_rcnn_d]\n",
    "\n",
    "print('object: {}, the {}-th ({:.3f}) RCNN prediction is selected for estimating 6D pose. '.format(\n",
    "    obj_name, rcnn_idx, tar_obj_scores[rcnn_idx]))\n",
    "print('RCNN confs: {}'.format(tar_obj_scores.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779dcfc",
   "metadata": {},
   "source": [
    "# Visualize the predicted pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ce0bb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_pose_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m obj_mesh \u001b[38;5;241m=\u001b[39m tar_obj_codebook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_mesh\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m PD_raw_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m4\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 6\u001b[0m PD_raw_pose[:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mraw_pose_t\u001b[49m\n\u001b[1;32m      7\u001b[0m PD_raw_pose[:\u001b[38;5;241m3\u001b[39m, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mPOSE_TO_BOP(raw_pose_R)\n\u001b[1;32m     10\u001b[0m PD_icp1_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m4\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_pose_t' is not defined"
     ]
    }
   ],
   "source": [
    "import misc \n",
    "\n",
    "obj_mesh = tar_obj_codebook['obj_mesh']\n",
    "\n",
    "PD_raw_pose = torch.eye(4, dtype=torch.float32)\n",
    "PD_raw_pose[:3, 3] = raw_pose_t\n",
    "PD_raw_pose[:3, :3] = cfg.POSE_TO_BOP(raw_pose_R)\n",
    "\n",
    "\n",
    "PD_icp1_pose = torch.eye(4, dtype=torch.float32)\n",
    "PD_icp1_pose[:3, 3] = icp1_pose_t\n",
    "PD_icp1_pose[:3, :3] = cfg.POSE_TO_BOP(icp1_pose_R)\n",
    "\n",
    "\n",
    "PD_icpk_pose = torch.eye(4, dtype=torch.float32)\n",
    "PD_icpk_pose[:3, 3] = icpk_pose_t\n",
    "PD_icpk_pose[:3, :3] = cfg.POSE_TO_BOP(icpk_pose_R)\n",
    "\n",
    "\n",
    "GT_pose = torch.eye(4, dtype=torch.float32)\n",
    "GT_pose[:3, :3] = obj_gt_R\n",
    "GT_pose[:3, 3] = obj_gt_t\n",
    "\n",
    "obj_diameter = tar_obj_codebook['diameter']\n",
    "corner_pts = obj_mesh.bounds  # object bounding box vertices\n",
    "obj_pcl = obj_mesh.vertices\n",
    "N_pcl = len(obj_pcl)\n",
    "select_obj_idxes = torch.randperm(N_pcl)[:500]\n",
    "select_obj_pts = torch.tensor(obj_pcl[select_obj_idxes])\n",
    "\n",
    "GT_2D_bbox = misc.box_2D_shape(points=corner_pts, pose=GT_pose, K=cam_K)\n",
    "PD_raw_2D_bbox = misc.box_2D_shape(points=corner_pts, pose=PD_raw_pose, K=cam_K)\n",
    "PD_icp1_2D_bbox = misc.box_2D_shape(points=corner_pts, pose=PD_icp1_pose, K=cam_K)\n",
    "PD_icpk_2D_bbox = misc.box_2D_shape(points=corner_pts, pose=PD_icpk_pose, K=cam_K)\n",
    "\n",
    "GT_shape = misc.bbox_to_shape(GT_2D_bbox.tolist())\n",
    "PD_raw_shape = misc.bbox_to_shape(PD_raw_2D_bbox.tolist())\n",
    "PD_icp1_shape = misc.bbox_to_shape(PD_icp1_2D_bbox.tolist())\n",
    "PD_icpk_shape = misc.bbox_to_shape(PD_icpk_2D_bbox.tolist())\n",
    "\n",
    "\n",
    "pil_img = Image.open(color_file)\n",
    "Raw_bbox_img = pil_img.copy()\n",
    "\n",
    "draw = ImageDraw.Draw(Raw_bbox_img)\n",
    "draw.line(GT_shape, (255, 0, 0), 3)      # Red 3D bbox for GT\n",
    "draw.line(PD_raw_shape, (0, 0, 255), 3)  # Blue 3D bbox for OVE6D\n",
    "\n",
    "\n",
    "ICP1_bbox_img = pil_img.copy()\n",
    "draw = ImageDraw.Draw(ICP1_bbox_img)\n",
    "draw.line(GT_shape, (255, 0, 0), 3)  # Red 3D bbox for GT\n",
    "draw.line(PD_icp1_shape, (0, 255, 0), 3)  # Green 3D bbox for OVE6D with ICP after pose selection\n",
    "\n",
    "\n",
    "ICPK_bbox_img = pil_img.copy()\n",
    "draw = ImageDraw.Draw(ICPK_bbox_img)\n",
    "draw.line(GT_shape, (255, 0, 0), 3)  # Red 3D bbox for GT\n",
    "draw.line(PD_icpk_shape, (255, 0, 255), 3)  # Cyan 3D bbox for OVE6D with ICP before pose selection\n",
    "\n",
    "\n",
    "##### calculate the pose errors ########\n",
    "raw_R_err = utils.rotation_error(raw_pose_R, obj_gt_R)\n",
    "icp1_R_err = utils.rotation_error(icp1_pose_R, obj_gt_R)\n",
    "icpk_R_err = utils.rotation_error(icpk_pose_R, obj_gt_R)\n",
    "\n",
    "raw_t_err = (((raw_pose_t - obj_gt_t)**2).sum())**0.5 * 1000\n",
    "icp1_t_err = (((icp1_pose_t - obj_gt_t)**2).sum())**0.5 * 1000\n",
    "icpk_t_err = (((icpk_pose_t - obj_gt_t)**2).sum())**0.5 * 1000\n",
    "\n",
    "\n",
    "raw_add_err = misc.add(R_est=raw_pose_R.numpy(), t_est=raw_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "raw_adi_err = misc.adi(R_est=raw_pose_R.numpy(), t_est=raw_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "\n",
    "icp1_add_err = misc.add(R_est=icp1_pose_R.numpy(), t_est=icp1_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "icp1_adi_err = misc.adi(R_est=icp1_pose_R.numpy(), t_est=icp1_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "\n",
    "icpk_add_err = misc.add(R_est=icpk_pose_R.numpy(), t_est=icpk_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "icpk_adi_err = misc.adi(R_est=icpk_pose_R.numpy(), t_est=icpk_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(30, 10))\n",
    "ax[0].imshow(Raw_bbox_img)\n",
    "ax[0].set_title(\"T(0.1d)={:.1f}mm, Raw R: {:.1f}˚, t: {:.1f}mm, add: {:.1f}mm, adi: {:.1f}mm\".format(\n",
    "    obj_diameter * 100, raw_R_err, raw_t_err, raw_add_err, raw_adi_err), fontsize=20)\n",
    "ax[1].imshow(ICP1_bbox_img)\n",
    "ax[1].set_title(\"T={:.1f}mm, ICP1 R: {:.1f}˚, t: {:.1f}mm, add: {:.1f}mm, adi: {:.1f}mm\".format(\n",
    "    obj_diameter * 100, icp1_R_err, icp1_t_err, icp1_add_err, icp1_adi_err), fontsize=20)\n",
    "ax[2].imshow(ICPK_bbox_img)\n",
    "ax[2].set_title(\"T={:.1f}mm, ICPK R: {:.1f}˚, t: {:.1f}mm, add: {:.1f}mm, adi: {:.1f}mm\".format(\n",
    "    obj_diameter * 100, icpk_R_err, icpk_t_err, icpk_add_err, icpk_adi_err), fontsize=20)\n",
    "ax[0].axis(False)\n",
    "ax[1].axis(False)\n",
    "ax[2].axis(False)\n",
    "\n",
    "# print('object diameter: {:.1f}mm, 0.1d:{:.1f}mm'.format(obj_diameter*1000, obj_diameter*100))\n",
    "print('GT(Red): \\t {}, diameter: {:.1f}mm, pose threshold(0.1d): {:.1f}mm'.format(obj_name, obj_diameter*1000, obj_diameter*100))\n",
    "print('Raw(Blue): \\t rot error:{:.1f}˚, tsl_err:{:.1f}mm, add:{:.1f}mm, adi:{:.1f}mm'.format(raw_R_err, raw_t_err, raw_add_err, raw_adi_err))\n",
    "print('ICP1(Green): \\t rot error:{:.1f}˚, tsl_err:{:.1f}mm, add:{:.1f}mm, adi:{:.1f}mm'.format(icp1_R_err, icp1_t_err, icp1_add_err, icp1_adi_err))\n",
    "print('ICPK(Cyan): \\t rot error:{:.1f}˚, tsl_err:{:.1f}mm, add:{:.1f}mm, adi:{:.1f}mm'.format(icpk_R_err, icpk_t_err, icpk_add_err, icpk_adi_err))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8dcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb54269f6222b6ab0b3eebeb4bda3d4fe55f11f569665d5647d56bc5c73b90b7"
  },
  "kernelspec": {
   "display_name": "ove6d",
   "language": "python",
   "name": "ove6d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
