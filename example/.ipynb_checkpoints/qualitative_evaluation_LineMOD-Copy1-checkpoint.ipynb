{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "useful-subscription",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "\n",
    "\n",
    "from os.path import join as pjoin\n",
    "from bop_toolkit_lib import inout\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base_path = os.path.dirname(os.path.abspath(\".\"))\n",
    "sys.path.append(base_path)\n",
    "\n",
    "from lib import rendering, network\n",
    "\n",
    "from dataset import LineMOD_Dataset, prototype_Dataset\n",
    "from evaluation import utils\n",
    "from evaluation import config as cfg\n",
    "\n",
    "gpu_id = 0\n",
    "# gpu_id = 1\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "os.environ['EGL_DEVICE_ID'] = str(gpu_id)\n",
    "DEVICE = torch.device('cuda')\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "\n",
    "\n",
    "datapath = Path(cfg.DATA_PATH)\n",
    "eval_dataset = prototype_Dataset.Dataset(datapath / 'huawei_box')\n",
    "#eval_dataset = LineMOD_Dataset.Dataset(datapath / 'lm')\n",
    "\n",
    "cfg.RENDER_WIDTH = eval_dataset.cam_width    # the width of rendered images\n",
    "cfg.RENDER_HEIGHT = eval_dataset.cam_height  # the height of rendered images\n",
    "cfg.DATASET_NAME = 'huawei_box'        # dataset name\n",
    "#cfg.DATASET_NAME = 'lm'        # dataset name\n",
    "\n",
    "cfg.HEMI_ONLY = True   # only the upper hemishpere is used for LineMOD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d46f50",
   "metadata": {},
   "source": [
    "# Load Mask-RCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd464b03-aed8-41b9-be10-a460c34d726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not needed for image test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "573ed547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask-RCNN has been loaded!\n"
     ]
    }
   ],
   "source": [
    "################################################# MASK-RCNN Segmentation ##################################################################\n",
    "#rcnnIdx_to_lmIds_dict = {0:1, 1:2, 2:3, 3:4, 4:5, 5:6, 6:7, 7:8, 8:9, 9:10, 10:11, 11:12, 12:13, 13:14, 14:15}\n",
    "#rcnnIdx_to_lmCats_dict ={0:'Ape', 1:'Benchvice', 2:'Bowl', 3:'Camera', 4:'Can', 5:'Cat', 6:'Cup', 7:'Driller', \n",
    "#                        8:'Duck', 9:'Eggbox', 10:'Glue', 11:'Holepunch', 12:'Iron', 13:'Lamp', 14:'Phone'}\n",
    "#rcnn_cfg = get_cfg()\n",
    "#rcnn_cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "#rcnn_cfg.MODEL.WEIGHTS = os.path.abspath(os.path.join(base_path, 'checkpoints','lm_maskrcnn_model.pth'))\n",
    "#rcnn_cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(rcnnIdx_to_lmCats_dict)\n",
    "#rcnn_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.001 # the predicted category scores\n",
    "#predictor = DefaultPredictor(rcnn_cfg)\n",
    "################################################## MASK-RCNN Segmentation ##################################################################\n",
    "#print('Mask-RCNN has been loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65717a2c",
   "metadata": {},
   "source": [
    "# Load OVE6D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24195e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVE6D has been loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ckpt_file = pjoin(base_path, \n",
    "                'checkpoints', \n",
    "                \"OVE6D_pose_model.pth\"\n",
    "                )\n",
    "model_net = network.OVE6D().to(DEVICE)\n",
    "\n",
    "model_net.load_state_dict(torch.load(ckpt_file))\n",
    "model_net.eval()\n",
    "print('OVE6D has been loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9571f",
   "metadata": {},
   "source": [
    "#  Load object viewpoint codebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9788e898-d3ef-44cf-bdcc-56d0e12419cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataspace/huawei_box/models_eval\n",
      "<module 'evaluation.config' from '/home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/config.py'>\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset.model_dir)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e2da40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  /home/nicklas/Projects/pose_demo/OVE6D-pose/evaluation/object_codebooks/huawei_box/zoom_0.01/views_4000/huawei_box_obj_01_views_4000.npy\n",
      "Object codebooks have been loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg.VIEWBOOK_BATCHSIZE = 200 # reduce this if out of GPU memory, \n",
    "codebook_saving_dir = pjoin(base_path,'evaluation/object_codebooks',\n",
    "                            cfg.DATASET_NAME, \n",
    "                            'zoom_{}'.format(cfg.ZOOM_DIST_FACTOR), \n",
    "                            'views_{}'.format(str(cfg.RENDER_NUM_VIEWS)))\n",
    "\n",
    "\n",
    "\n",
    "object_codebooks = utils.OVE6D_codebook_generation(codebook_dir=codebook_saving_dir, \n",
    "                                                    model_func=model_net,\n",
    "                                                    dataset=eval_dataset, \n",
    "                                                    config=cfg, \n",
    "                                                    device=DEVICE)\n",
    "print('Object codebooks have been loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdcb098",
   "metadata": {},
   "source": [
    "# Object segmentation and pose estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33a0eed3-6c84-48a5-b6ac-b33020c21e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 128, 60]) torch.Size([50, 128, 60])\n",
      "torch.Size([50, 60, 60])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "b = 50\n",
    "c = 128\n",
    "d = 60\n",
    "\n",
    "x = t.rand(b,c,d).to('cuda')\n",
    "y = t.rand(b,c,d).to('cuda')\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "z = t.bmm(x.permute(0,2,1), y)\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39410384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 128, 60]) torch.Size([50, 128, 60])\n",
      "> \u001b[0;32m/home/nicklas/Projects/pose_demo/OVE6D-pose/lib/network.py\u001b[0m(167)\u001b[0;36mregression_head\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    165 \u001b[0;31m        \u001b[0;31m#y = y.to('cpu')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    166 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 167 \u001b[0;31m        \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# BV x 64 x 64, feature map correlation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    168 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    169 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/nicklas/Projects/pose_demo/OVE6D-pose/lib/network.py\u001b[0m(169)\u001b[0;36mregression_head\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    167 \u001b[0;31m        \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# BV x 64 x 64, feature map correlation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    168 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 169 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    170 \u001b[0;31m        \u001b[0;31m########################################################### debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    171 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/nicklas/Projects/pose_demo/OVE6D-pose/lib/network.py\u001b[0m(172)\u001b[0;36mregression_head\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    170 \u001b[0;31m        \u001b[0;31m########################################################### debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    171 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 172 \u001b[0;31m        \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# BV x 64 x 64, feature map correlation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    173 \u001b[0;31m        \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# BV x 4096\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    174 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`\n",
      "> \u001b[0;32m/home/nicklas/Projects/pose_demo/OVE6D-pose/lib/network.py\u001b[0m(172)\u001b[0;36mregression_head\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    170 \u001b[0;31m        \u001b[0;31m########################################################### debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    171 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 172 \u001b[0;31m        \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# BV x 64 x 64, feature map correlation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    173 \u001b[0;31m        \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# BV x 4096\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    174 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  torch.mm(x.permute(0,2,1)[0],y[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[30.8725, 31.1470, 32.8982,  ..., 34.3447, 33.0740, 33.5284],\n",
      "        [33.2064, 32.5210, 37.5676,  ..., 37.0851, 35.6170, 34.4687],\n",
      "        [29.6675, 30.1239, 33.7423,  ..., 35.0381, 33.8905, 31.1955],\n",
      "        ...,\n",
      "        [28.8669, 31.4411, 32.1517,  ..., 33.5450, 32.0924, 31.0351],\n",
      "        [31.9299, 34.0325, 35.8756,  ..., 36.3667, 35.9386, 35.8058],\n",
      "        [26.9647, 28.3821, 32.0270,  ..., 30.4680, 30.1868, 31.0502]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  torch.device('gpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** RuntimeError: Expected one of cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, msnpu, xla, vulkan device type at start of device string: gpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg.VP_NUM_TOPK = 50   # the retrieval number of viewpoint \n",
    "cfg.RANK_NUM_TOPK = 5  # the ranking number of full 3D orientation \n",
    "\n",
    "\n",
    "\n",
    "test_data_dir = datapath / 'lm' / 'test'          # path to the test dataset of BOP\n",
    "obj_renderer = rendering.Renderer(width=cfg.RENDER_WIDTH, height=cfg.RENDER_HEIGHT)\n",
    "\n",
    "scene_id = 1   # range [1, 15]\n",
    "view_id = 0  # range [0, 1214]\n",
    "\n",
    "scene_dir = pjoin(test_data_dir, '{:06d}'.format(scene_id))\n",
    "color_file = pjoin(scene_dir, 'rgb/{:06d}.png'.format(view_id))\n",
    "depth_file = pjoin(scene_dir, 'depth/{:06d}.png'.format(view_id))\n",
    "mask_file = pjoin(scene_dir, 'mask_visib/{:06d}_{:06d}.png'.format(view_id, 0))\n",
    "\n",
    "tar_obj_id = scene_id # object id equals the scene id for LM dataset\n",
    "tar_obj_codebook = object_codebooks[tar_obj_id]\n",
    "\n",
    "############## read the camera information ##############\n",
    "cam_info_file = pjoin(scene_dir, 'scene_camera.json')\n",
    "with open(cam_info_file, 'r') as cam_f:\n",
    "    scene_camera_info = json.load(cam_f)\n",
    "view_cam_info = scene_camera_info[str(view_id)]  # scene camera information        \n",
    "\n",
    "\n",
    "############## read the ground truth pose for calculating errors ##############\n",
    "gt_pose_file = os.path.join(scene_dir, 'scene_gt.json')\n",
    "with open(gt_pose_file, 'r') as pose_f:\n",
    "    pose_anno = json.load(pose_f)\n",
    "view_pose = pose_anno[str(view_id)][0]\n",
    "obj_gt_R = cfg.BOP_REF_POSE(torch.tensor(view_pose['cam_R_m2c'], dtype=torch.float32).view(3, 3))\n",
    "obj_gt_t = torch.tensor(view_pose['cam_t_m2c'], dtype=torch.float32) * cfg.MODEL_SCALING\n",
    "\n",
    "\n",
    "############## read the depth images and covert it from meter to millimeter ##############\n",
    "view_depth = torch.tensor(np.array(Image.open(depth_file)), dtype=torch.float32) # HxW\n",
    "view_depth *= view_cam_info['depth_scale']\n",
    "view_depth *= cfg.MODEL_SCALING # convert to meter scale from millimeter scale\n",
    "view_camK = torch.tensor(view_cam_info['cam_K'], dtype=torch.float32).view(3, 3)[None, ...] # 1x3x3\n",
    "cam_K = view_camK.to(DEVICE)\n",
    "view_depth = view_depth.to(DEVICE)\n",
    "\n",
    "\n",
    "############## read rgb image for object segmentation ##############\n",
    "\n",
    "## TODO:\n",
    "view_timer = time.time() \n",
    "rgb_img = cv2.imread(color_file)\n",
    "imread_cost = time.time() - view_timer\n",
    "\n",
    "rcnn_timer = time.time()\n",
    "output = predictor(rgb_img)\n",
    "rcnn_pred_ids = output[\"instances\"].pred_classes\n",
    "rcnn_pred_masks = output[\"instances\"].pred_masks\n",
    "rcnn_pred_scores = output[\"instances\"].scores\n",
    "rcnn_cost = time.time() - rcnn_timer\n",
    "###################### object segmentation ######################\n",
    "\n",
    "tar_rcnn_d = tar_obj_id - 1\n",
    "obj_masks = rcnn_pred_masks # NxHxW\n",
    "obj_depths = view_depth[None, ...] * obj_masks\n",
    "tar_obj_depths = obj_depths[tar_rcnn_d==rcnn_pred_ids]\n",
    "tar_obj_masks = rcnn_pred_masks[tar_rcnn_d==rcnn_pred_ids]\n",
    "tar_obj_scores = rcnn_pred_scores[tar_rcnn_d==rcnn_pred_ids]\n",
    "\n",
    "mask_pixel_count = tar_obj_masks.view(tar_obj_masks.size(0), -1).sum(dim=1)\n",
    "valid_idx = (mask_pixel_count >= 100)\n",
    "if valid_idx.sum() == 0:\n",
    "    mask_visib_ratio = mask_pixel_count / mask_pixel_count.max()\n",
    "    valid_idx = mask_visib_ratio >= 0.05\n",
    "\n",
    "tar_obj_masks = tar_obj_masks[valid_idx] # select the target object instance masks\n",
    "tar_obj_depths = tar_obj_depths[valid_idx]\n",
    "tar_obj_scores = tar_obj_scores[valid_idx]\n",
    "\n",
    "pose_ret, rcnn_idx = utils.OVE6D_rcnn_full_pose(model_func=model_net, \n",
    "                                    obj_depths=tar_obj_depths,\n",
    "                                    obj_masks=tar_obj_masks,\n",
    "                                    obj_rcnn_scores=tar_obj_scores,\n",
    "                                    obj_codebook=tar_obj_codebook, \n",
    "                                    cam_K=cam_K,\n",
    "                                    config=cfg, \n",
    "                                    device=DEVICE,\n",
    "                                    obj_renderer=obj_renderer, \n",
    "                                    return_rcnn_idx=True\n",
    "                                    )\n",
    "del obj_renderer\n",
    "\n",
    "raw_pose_R = pose_ret['raw_R'] # without ICP\n",
    "raw_pose_t = pose_ret['raw_t'] # without ICP\n",
    "\n",
    "icp1_pose_R = pose_ret['icp1_R'] # with ICP after pose selection\n",
    "icp1_pose_t = pose_ret['icp1_t'] # with ICP after pose selection\n",
    "\n",
    "icpk_pose_R = pose_ret['icpk_R'] # with ICP before pose selection\n",
    "icpk_pose_t = pose_ret['icpk_t'] # with ICP before pose selection\n",
    "\n",
    "plt.imshow(tar_obj_masks.sum(dim=0).cpu())\n",
    "plt.axis(False)\n",
    "\n",
    "obj_name = rcnnIdx_to_lmCats_dict[tar_rcnn_d]\n",
    "\n",
    "print('object: {}, the {}-th ({:.3f}) RCNN prediction is selected for estimating 6D pose. '.format(\n",
    "    obj_name, rcnn_idx, tar_obj_scores[rcnn_idx]))\n",
    "print('RCNN confs: {}'.format(tar_obj_scores.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779dcfc",
   "metadata": {},
   "source": [
    "# Visualize the predicted pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import misc \n",
    "\n",
    "obj_mesh = tar_obj_codebook['obj_mesh']\n",
    "\n",
    "PD_raw_pose = torch.eye(4, dtype=torch.float32)\n",
    "PD_raw_pose[:3, 3] = raw_pose_t\n",
    "PD_raw_pose[:3, :3] = cfg.POSE_TO_BOP(raw_pose_R)\n",
    "\n",
    "\n",
    "PD_icp1_pose = torch.eye(4, dtype=torch.float32)\n",
    "PD_icp1_pose[:3, 3] = icp1_pose_t\n",
    "PD_icp1_pose[:3, :3] = cfg.POSE_TO_BOP(icp1_pose_R)\n",
    "\n",
    "\n",
    "PD_icpk_pose = torch.eye(4, dtype=torch.float32)\n",
    "PD_icpk_pose[:3, 3] = icpk_pose_t\n",
    "PD_icpk_pose[:3, :3] = cfg.POSE_TO_BOP(icpk_pose_R)\n",
    "\n",
    "\n",
    "GT_pose = torch.eye(4, dtype=torch.float32)\n",
    "GT_pose[:3, :3] = obj_gt_R\n",
    "GT_pose[:3, 3] = obj_gt_t\n",
    "\n",
    "obj_diameter = tar_obj_codebook['diameter']\n",
    "corner_pts = obj_mesh.bounds  # object bounding box vertices\n",
    "obj_pcl = obj_mesh.vertices\n",
    "N_pcl = len(obj_pcl)\n",
    "select_obj_idxes = torch.randperm(N_pcl)[:500]\n",
    "select_obj_pts = torch.tensor(obj_pcl[select_obj_idxes])\n",
    "\n",
    "GT_2D_bbox = misc.box_2D_shape(points=corner_pts, pose=GT_pose, K=cam_K)\n",
    "PD_raw_2D_bbox = misc.box_2D_shape(points=corner_pts, pose=PD_raw_pose, K=cam_K)\n",
    "PD_icp1_2D_bbox = misc.box_2D_shape(points=corner_pts, pose=PD_icp1_pose, K=cam_K)\n",
    "PD_icpk_2D_bbox = misc.box_2D_shape(points=corner_pts, pose=PD_icpk_pose, K=cam_K)\n",
    "\n",
    "GT_shape = misc.bbox_to_shape(GT_2D_bbox.tolist())\n",
    "PD_raw_shape = misc.bbox_to_shape(PD_raw_2D_bbox.tolist())\n",
    "PD_icp1_shape = misc.bbox_to_shape(PD_icp1_2D_bbox.tolist())\n",
    "PD_icpk_shape = misc.bbox_to_shape(PD_icpk_2D_bbox.tolist())\n",
    "\n",
    "\n",
    "pil_img = Image.open(color_file)\n",
    "Raw_bbox_img = pil_img.copy()\n",
    "\n",
    "draw = ImageDraw.Draw(Raw_bbox_img)\n",
    "draw.line(GT_shape, (255, 0, 0), 3)      # Red 3D bbox for GT\n",
    "draw.line(PD_raw_shape, (0, 0, 255), 3)  # Blue 3D bbox for OVE6D\n",
    "\n",
    "\n",
    "ICP1_bbox_img = pil_img.copy()\n",
    "draw = ImageDraw.Draw(ICP1_bbox_img)\n",
    "draw.line(GT_shape, (255, 0, 0), 3)  # Red 3D bbox for GT\n",
    "draw.line(PD_icp1_shape, (0, 255, 0), 3)  # Green 3D bbox for OVE6D with ICP after pose selection\n",
    "\n",
    "\n",
    "ICPK_bbox_img = pil_img.copy()\n",
    "draw = ImageDraw.Draw(ICPK_bbox_img)\n",
    "draw.line(GT_shape, (255, 0, 0), 3)  # Red 3D bbox for GT\n",
    "draw.line(PD_icpk_shape, (255, 0, 255), 3)  # Cyan 3D bbox for OVE6D with ICP before pose selection\n",
    "\n",
    "\n",
    "##### calculate the pose errors ########\n",
    "raw_R_err = utils.rotation_error(raw_pose_R, obj_gt_R)\n",
    "icp1_R_err = utils.rotation_error(icp1_pose_R, obj_gt_R)\n",
    "icpk_R_err = utils.rotation_error(icpk_pose_R, obj_gt_R)\n",
    "\n",
    "raw_t_err = (((raw_pose_t - obj_gt_t)**2).sum())**0.5 * 1000\n",
    "icp1_t_err = (((icp1_pose_t - obj_gt_t)**2).sum())**0.5 * 1000\n",
    "icpk_t_err = (((icpk_pose_t - obj_gt_t)**2).sum())**0.5 * 1000\n",
    "\n",
    "\n",
    "raw_add_err = misc.add(R_est=raw_pose_R.numpy(), t_est=raw_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "raw_adi_err = misc.adi(R_est=raw_pose_R.numpy(), t_est=raw_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "\n",
    "icp1_add_err = misc.add(R_est=icp1_pose_R.numpy(), t_est=icp1_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "icp1_adi_err = misc.adi(R_est=icp1_pose_R.numpy(), t_est=icp1_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "\n",
    "icpk_add_err = misc.add(R_est=icpk_pose_R.numpy(), t_est=icpk_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "icpk_adi_err = misc.adi(R_est=icpk_pose_R.numpy(), t_est=icpk_pose_t.numpy(), \n",
    "                        R_gt=obj_gt_R.numpy(), t_gt=obj_gt_t.numpy(), \n",
    "                        pts=select_obj_pts.numpy())*1000\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(30, 10))\n",
    "ax[0].imshow(Raw_bbox_img)\n",
    "ax[0].set_title(\"T(0.1d)={:.1f}mm, Raw R: {:.1f}˚, t: {:.1f}mm, add: {:.1f}mm, adi: {:.1f}mm\".format(\n",
    "    obj_diameter * 100, raw_R_err, raw_t_err, raw_add_err, raw_adi_err), fontsize=20)\n",
    "ax[1].imshow(ICP1_bbox_img)\n",
    "ax[1].set_title(\"T={:.1f}mm, ICP1 R: {:.1f}˚, t: {:.1f}mm, add: {:.1f}mm, adi: {:.1f}mm\".format(\n",
    "    obj_diameter * 100, icp1_R_err, icp1_t_err, icp1_add_err, icp1_adi_err), fontsize=20)\n",
    "ax[2].imshow(ICPK_bbox_img)\n",
    "ax[2].set_title(\"T={:.1f}mm, ICPK R: {:.1f}˚, t: {:.1f}mm, add: {:.1f}mm, adi: {:.1f}mm\".format(\n",
    "    obj_diameter * 100, icpk_R_err, icpk_t_err, icpk_add_err, icpk_adi_err), fontsize=20)\n",
    "ax[0].axis(False)\n",
    "ax[1].axis(False)\n",
    "ax[2].axis(False)\n",
    "\n",
    "# print('object diameter: {:.1f}mm, 0.1d:{:.1f}mm'.format(obj_diameter*1000, obj_diameter*100))\n",
    "print('GT(Red): \\t {}, diameter: {:.1f}mm, pose threshold(0.1d): {:.1f}mm'.format(obj_name, obj_diameter*1000, obj_diameter*100))\n",
    "print('Raw(Blue): \\t rot error:{:.1f}˚, tsl_err:{:.1f}mm, add:{:.1f}mm, adi:{:.1f}mm'.format(raw_R_err, raw_t_err, raw_add_err, raw_adi_err))\n",
    "print('ICP1(Green): \\t rot error:{:.1f}˚, tsl_err:{:.1f}mm, add:{:.1f}mm, adi:{:.1f}mm'.format(icp1_R_err, icp1_t_err, icp1_add_err, icp1_adi_err))\n",
    "print('ICPK(Cyan): \\t rot error:{:.1f}˚, tsl_err:{:.1f}mm, add:{:.1f}mm, adi:{:.1f}mm'.format(icpk_R_err, icpk_t_err, icpk_add_err, icpk_adi_err))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8dcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb54269f6222b6ab0b3eebeb4bda3d4fe55f11f569665d5647d56bc5c73b90b7"
  },
  "kernelspec": {
   "display_name": "ove6d",
   "language": "python",
   "name": "ove6d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
